{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfFyAL3o4RA2"
      },
      "outputs": [],
      "source": [
        "# utils_text.py\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic word-level tokenizer:\n",
        "    - lowercases\n",
        "    - keeps words and basic punctuation as separate tokens\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # separate punctuation\n",
        "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def load_poems(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def build_vocab(tokens: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    counts = Counter(tokens)\n",
        "    vocab = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    for w, c in counts.items():\n",
        "        if c >= min_freq and w not in vocab:\n",
        "            vocab.append(w)\n",
        "    stoi = {w: i for i, w in enumerate(vocab)}\n",
        "    itos = {i: w for w, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def tokens_to_ids(tokens: List[str], stoi: Dict[str, int]) -> List[int]:\n",
        "    unk = stoi[\"<unk>\"]\n",
        "    return [stoi.get(t, unk) for t in tokens]\n",
        "\n",
        "\n",
        "def make_sequences(ids: List[int], seq_len: int) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Make (X, Y) where Y is next-token targets.\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    for i in range(0, len(ids) - seq_len):\n",
        "        x = ids[i:i+seq_len]\n",
        "        y = ids[i+1:i+seq_len+1]\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "j_L5KbeQ4r-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scratch_rnn_numpy.py\n",
        "import numpy as np\n",
        "# Moved imports and functions from utils_text.py\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic word-level tokenizer:\n",
        "    - lowercases\n",
        "    - keeps words and basic punctuation as separate tokens\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # separate punctuation\n",
        "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def load_poems(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def build_vocab(tokens: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    counts = Counter(tokens)\n",
        "    vocab = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    for w, c in counts.items():\n",
        "        if c >= min_freq and w not in vocab:\n",
        "            vocab.append(w)\n",
        "    stoi = {w: i for i, w in enumerate(vocab)}\n",
        "    itos = {i: w for w, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def tokens_to_ids(tokens: List[str], stoi: Dict[str, int]) -> List[int]:\n",
        "    unk = stoi[\"<unk>\"]\n",
        "    return [stoi.get(t, unk) for t in tokens]\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "\n",
        "def one_hot(idx, V):\n",
        "    v = np.zeros((V, 1))\n",
        "    v[idx] = 1.0\n",
        "    return v\n",
        "\n",
        "\n",
        "class ScratchRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=64, lr=1e-2, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.V = vocab_size\n",
        "        self.H = hidden_size\n",
        "        self.lr = lr\n",
        "\n",
        "        # weights\n",
        "        self.Wxh = rng.normal(0, 0.01, (self.H, self.V))\n",
        "        self.Whh = rng.normal(0, 0.01, (self.H, self.H))\n",
        "        self.Why = rng.normal(0, 0.01, (self.V, self.H))\n",
        "        self.bh = np.zeros((self.H, 1))\n",
        "        self.by = np.zeros((self.V, 1))\n",
        "\n",
        "    def forward(self, inputs, hprev):\n",
        "        \"\"\"\n",
        "        inputs: list of token indices length T\n",
        "        returns cache for backprop\n",
        "        \"\"\"\n",
        "        xs, hs, ys, ps = {}, {}, {}, {}\n",
        "        hs[-1] = hprev\n",
        "\n",
        "        for t, idx in enumerate(inputs):\n",
        "            xs[t] = one_hot(idx, self.V)                         # [V,1]\n",
        "            hs[t] = np.tanh(self.Wxh @ xs[t] + self.Whh @\n",
        "                            hs[t-1] + self.bh)  # [H,1]\n",
        "            ys[t] = self.Why @ hs[t] + self.by                  # [V,1]\n",
        "            ps[t] = softmax(ys[t].ravel()).reshape(-1, 1)       # [V,1]\n",
        "        return xs, hs, ps\n",
        "\n",
        "    def loss_and_grads(self, inputs, targets, hprev):\n",
        "        xs, hs, ps = self.forward(inputs, hprev)\n",
        "\n",
        "        loss = 0.0\n",
        "        for t in range(len(inputs)):\n",
        "            loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
        "\n",
        "        # grads init\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        dhnext = np.zeros((self.H, 1))\n",
        "\n",
        "        for t in reversed(range(len(inputs))):\n",
        "            dy = ps[t].copy()\n",
        "            # softmax CE gradient\n",
        "            dy[targets[t]] -= 1.0\n",
        "            dWhy += dy @ hs[t].T\n",
        "            dby += dy\n",
        "\n",
        "            dh = self.Why.T @ dy + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh                    # tanh'\n",
        "            dbh += dhraw\n",
        "            dWxh += dhraw @ xs[t].T\n",
        "            dWhh += dhraw @ hs[t-1].T\n",
        "            dhnext = self.Whh.T @ dhraw\n",
        "\n",
        "        # clip\n",
        "        for d in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(d, -5, 5, out=d)\n",
        "\n",
        "        hlast = hs[len(inputs)-1]\n",
        "        return loss, (dWxh, dWhh, dWhy, dbh, dby), hlast\n",
        "\n",
        "    def step(self, grads):\n",
        "        dWxh, dWhh, dWhy, dbh, dby = grads\n",
        "        self.Wxh -= self.lr * dWxh\n",
        "        self.Whh -= self.lr * dWhh\n",
        "        self.Why -= self.lr * dWhy\n",
        "        self.bh -= self.lr * dbh\n",
        "        self.by -= self.lr * dby\n",
        "\n",
        "    def sample(self, start_idx, itos, length=30, temperature=1.0):\n",
        "        h = np.zeros((self.H, 1))\n",
        "        x = one_hot(start_idx, self.V)\n",
        "        out = []\n",
        "\n",
        "        for _ in range(length):\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            p = softmax((y.ravel() / max(temperature, 1e-6)))\n",
        "            idx = np.random.choice(range(self.V), p=p)\n",
        "            out.append(itos[idx])\n",
        "            x = one_hot(idx, self.V)\n",
        "        return \" \".join(out)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    rnn = ScratchRNN(vocab_size=len(stoi), hidden_size=128, lr=0.05)\n",
        "    seq_len = 25\n",
        "    h = np.zeros((rnn.H, 1))\n",
        "\n",
        "    # train a bit\n",
        "    for epoch in range(10):\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "        for i in range(0, len(ids) - seq_len - 1, seq_len):\n",
        "            inp = ids[i:i+seq_len]\n",
        "            tgt = ids[i+1:i+seq_len+1]\n",
        "            loss, grads, h = rnn.loss_and_grads(inp, tgt, h)\n",
        "            rnn.step(grads)\n",
        "            total_loss += loss\n",
        "            n += 1\n",
        "\n",
        "        avg = total_loss / max(n, 1)\n",
        "        print(f\"Epoch {epoch+1} | avg loss: {avg:.4f}\")\n",
        "        print(\"Sample:\", rnn.sample(\n",
        "            stoi[\"<bos>\"], itos, length=30, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSkVSakV4XWG",
        "outputId": "7e72469e-c79b-4813-b6ec-0c597f475ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | avg loss: 287.0521\n",
            "Sample: cider-mill prevailed have , , and when have the , , of this have have , , figured this have i possesses , when this have have , , of\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 | avg loss: 292.5206\n",
            "Sample: only crowd both , , emma this by place , , emma this have place , , emma this have flung , , and this two cause , , emma\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 | avg loss: 291.8282\n",
            "Sample: immodest see discover , , or this have place , , when multitude have salt , , of at wild i , , when higher have move , , perhaps\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 | avg loss: 291.5557\n",
            "Sample: articulation – i , , emma and and splendid , , perhaps and and have , , gazed and and natural , , emma and and but , , emma\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 | avg loss: 289.6390\n",
            "Sample: homely yo leave , , emma this have large , , seat this wild place , , emma this have wild , , seen this crier cause , of when\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 | avg loss: 291.8300\n",
            "Sample: fro like have , , emma ' have rock , , when this have days , , emma this have have , of emma ' have wild , , perhaps\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 | avg loss: 291.3338\n",
            "Sample: baptized birds our our , emma this this our night-wandering , emma this this our produce , emma this this our our , when this wild our our , when\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 | avg loss: 290.1668\n",
            "Sample: big reach our name , emma this this cause name , emma this wild the name , of this this our missing , emma this this our dedicate , of\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 | avg loss: 291.1341\n",
            "Sample: ’ loved the , , emma this have the , , emma this have after , , emma this have place , , emma this wild the , of emma\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 | avg loss: 289.2256\n",
            "Sample: prolific side supper , , or this wild place , , emma pierce wild i , , or this have have , , of this wild have , , or\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN One Hot"
      ],
      "metadata": {
        "id": "2wP-yvyh4emN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_torch_onehot.py\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#from utils import load_poems, simple_tokenize, build_vocab, tokens_to_ids, make_sequences\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available(\n",
        ") else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class SeqDatasetOneHot(Dataset):\n",
        "    def __init__(self, X, Y, vocab_size):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "        self.V = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_ids = self.X[idx]                    # [T]\n",
        "        y_ids = self.Y[idx]                    # [T]\n",
        "        # one-hot: [T, V]\n",
        "        x_oh = torch.zeros(x_ids.size(0), self.V, dtype=torch.float32)\n",
        "        x_oh.scatter_(1, x_ids.unsqueeze(1), 1.0)\n",
        "        return x_oh, y_ids\n",
        "\n",
        "\n",
        "class OneHotRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden=256):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=vocab_size,\n",
        "                          hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_oh, h0=None):\n",
        "        out, hn = self.rnn(x_oh, h0)        # out: [B,T,H]\n",
        "        logits = self.fc(out)               # [B,T,V]\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "    V = len(stoi)\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor(ids[-1:], dtype=torch.long,\n",
        "                         device=DEVICE)  # last token\n",
        "        x_oh = torch.zeros(1, 1, V, device=DEVICE)\n",
        "        x_oh.scatter_(2, x.view(1, 1, 1), 1.0)\n",
        "\n",
        "        logits, h = model(x_oh, h)\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    words = [itos[i] for i in ids]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetOneHot(X, Y, vocab_size=len(stoi))\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = OneHotRNNLM(vocab_size=len(stoi), hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training One-Hot RNN on\", DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_oh, y in dl:\n",
        "            x_oh = x_oh.to(DEVICE)      # [B,T,V]\n",
        "            y = y.to(DEVICE)            # [B,T]\n",
        "\n",
        "            logits, _ = model(x_oh)     # [B,T,V]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "        print(\"Sample:\", generate(model, stoi, itos,\n",
        "              seed_text=\"<bos>\", max_new=40, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Total training time (one-hot): {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox4djxDq4XLK",
        "outputId": "f54a2afa-0d60-412f-d889-4907044aceef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training One-Hot RNN on cpu\n",
            "Epoch 1 | loss: 6.2500\n",
            "Sample: <bos> trees , and will is all , dare and it \" and my crisis and been these they all the night of be bloody . not its so have , every breath . to the – broken ' not god\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 | loss: 5.0887\n",
            "Sample: <bos> over much . ) i care thee ! something you you shall of so best said have this who could alone them ! how i do not know not our juice goes up by the earth of rigging , and\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 | loss: 4.0377\n",
            "Sample: <bos> in my performers or quivering , the shade men ; , but single seems sweet it nature , from the fellow-senses holding , and forthwith , and was oh with offering as , purposes that hath pass a deck-hands ceaseless\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 | loss: 3.0173\n",
            "Sample: <bos> . ( \" ve now , no heart and cease , her ; for me that give me soft o while men and gasp happens ; they are something for you must as what ? you will say me .\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 | loss: 1.9751\n",
            "Sample: <bos> stand death and so you will not thee . i know i am kiss ' d , not we be more ! yet one ‘i different our transparent so of yourself and full of melons life toward as to you\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Embedding"
      ],
      "metadata": {
        "id": "MyIUfSbc4hGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_torch_embedding.py\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from utils import load_poems, simple_tokenize, build_vocab, tokens_to_ids, make_sequences\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available(\n",
        ") else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class SeqDatasetIdx(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]     # both [T]\n",
        "\n",
        "\n",
        "class EmbRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=128, hidden=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb)\n",
        "        self.rnn = nn.RNN(input_size=emb, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_ids, h0=None):\n",
        "        x = self.emb(x_ids)         # [B,T,E]\n",
        "        out, hn = self.rnn(x, h0)   # [B,T,H]\n",
        "        logits = self.fc(out)       # [B,T,V]\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor([[ids[-1]]], dtype=torch.long, device=DEVICE)  # [1,1]\n",
        "        logits, h = model(x, h)  # logits: [1,1,V]\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    return \" \".join(itos[i] for i in ids)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetIdx(X, Y)\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = EmbRNNLM(vocab_size=len(stoi), emb=128, hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training Embedding RNN on\", DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_ids, y in dl:\n",
        "            x_ids = x_ids.to(DEVICE)  # [B,T]\n",
        "            y = y.to(DEVICE)          # [B,T]\n",
        "\n",
        "            logits, _ = model(x_ids)  # [B,T,V]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "        print(\"Sample:\", generate(model, stoi, itos,\n",
        "              seed_text=\"<bos>\", max_new=40, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Total training time (embedding): {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "sxTrm43m4XGP",
        "outputId": "1ad99b9c-f274-4453-b35d-81a71fdf8822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_poems' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1414006345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1414006345.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_poems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"poems.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<bos>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msimple_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_poems' is not defined"
          ]
        }
      ]
    }
  ]
}